/* 
What it can do:

 * Learn, partially or completely unlearn/forget
 * Share knowledge: download upload and merge with "external knowledge" - people LLMs - SETI style
 * Fragmental  knowledge Loading by given context (kind of experts MoE)
 * Train/learn continually, in HW - in parrallel
 * Train - infer (generate) simultaneously
 * work with sequential or tabular data - mix of both categorical and numerical
 * 
 * Leak synaptic weight = sleep (day and night modes)
 * Be curious
 * Long attention (short attention is maxsynapses)
 * Short attention - single layer, complex multi-token relationships with multi-layers
 * Generate next, previous or any middle token at the same time 
 * 
 * Generate by frequency, reliability, context, ... whatever (CL, RL)
 * Train categorical (text) or numerical inputs
 * Train on multimodal inputs
 * Generate, classify, associatevly memorize, compare long sequences ~1 mil components, clusterize, mine MCLS
 * 
 * All on the naive edge LIF neuron implementation (fully convex - no gaps)
 * 
 * Maybe on 2D - not completed
 */


